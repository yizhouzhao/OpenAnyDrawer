{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee6ad60-2744-4dac-9eed-067aa7589976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2045fe3-08bd-4aa9-bad0-ab7b84402884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhou/miniconda3/envs/cliport/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, ResNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3a54db-3859-4cfa-bbc7-e7316e8ff2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6eb1bfa-8ee0-4376-87b0-509d6b66f1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: cats-image/image\n",
      "Reusing dataset cats-image (/home/yizhou/.cache/huggingface/datasets/huggingface___cats-image/image/1.9.0/68fbc793fb10cd165e490867f5d61fa366086ea40c73e549a020103dcb4f597e)\n",
      "100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 828.59it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab229b49-14fc-428c-ab6f-e7956cb05a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/resnet-18 were not used when initializing ResNetModel: ['classifier.1.weight', 'classifier.1.bias']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-18\")\n",
    "model = ResNetModel.from_pretrained(\"microsoft/resnet-18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180181e0-e7d2-4f98-a359-da4433f77548",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d46306-db9b-48ac-8294-0de0552aaa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4727aa0e-c00b-4d85-99f4-40b172f5ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6b4581-09ea-4114-8d11-c491f2644915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5dc6908-cdbb-4dc3-bdb3-f3cf411ac82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_cliport import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e7e4ba7-ca63-424c-8b8a-91a5e57ffdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e743202-df45-4ade-9ed8-6827109d649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = tokenizer([\"a photo of a cat\"], padding=True, return_tensors=\"pt\")\n",
    "text_features = model.get_text_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c98294f-6cf5-4d4a-9bfe-d5d3a9fa753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c91ae31-6bd9-48e9-89b0-f34da8e957fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11cf1c49-220d-4398-bc8f-04a8b4300d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CustomCliport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e272251-8b30-4161-b601-25471b767c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 7, 7]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b6f0d45-e661-4b12-b14b-4e2cd6ac77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cc(x, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d423d68-c519-47e5-bb1f-c16de8d10469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 224, 224])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bffe43ce-725d-477c-af32-e0033fa190a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.interpolate(x, size=(256, 256), mode='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9164e0c-7b21-4ff5-aa3b-4b571f0d5cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67df042b-1edf-4703-b50b-1fe9f977b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fc6c03a-210e-4f1d-86f2-74cf7839fe7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2587633"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "547deea2-92cb-41ae-9fca-f8661f0b499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SEMANTIC_TYPES = [f\"{v_desc}_{h_desc}_{cabinet_type}\" for v_desc in [\"\", \"bottom\", \"second-bottom\", \"middle\", \"second-top\", \"top\"] for h_desc in [\"\", \"right\", \"second-right\", \"middle\", \"second-left\", \"left\"] for cabinet_type in [\"drawer\", \"door\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8687c832-8883-4685-bee2-93d24f3b0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = [t.replace(\"_\",\" \").replace(\"-\",\" \").replace(\"  \", \" \").strip() for t in ALL_SEMANTIC_TYPES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d18b2311-3a3b-4ab7-a49c-bedf48619b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118de92-e9cf-4712-8da5-653324de380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a65f115-cdfc-4dbe-8d5f-90494f60c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(all_texts, padding=True, return_tensors=\"pt\")\n",
    "text_features = model.get_text_features(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86c51a47-d258-41b7-95ed-6de60c205eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2feature = {all_texts[i]: text_features[i].data for i in range(72)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "838174f9-21da-4212-b595-7840e34032e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.3948)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((text2feature['drawer'] - text2feature['right drawer'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a80f5b6a-94e4-4c8d-b185-87226e27a47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43.5977)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((text2feature['drawer'] - text2feature['middle door'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a415083-9a4b-4185-8b42-700e33d7ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save dictionary to pickle file\n",
    "with open('text2clip_feature.pickle', 'wb') as file:\n",
    "    pickle.dump(text2feature, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb9df7-35f5-4611-b2ed-df7ac4dcc40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cliport",
   "language": "python",
   "name": "cliport"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
